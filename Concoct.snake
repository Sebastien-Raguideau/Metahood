import os.path
import numpy as np
from collections import Counter
from Bio.SeqIO.FastaIO import SimpleFastaParser

# ---- prodigal annotation on splits ----------------------------------------------------------------
rule prodigal:
    input:
        "{group}/contigs/contigs.fa"
    output:
        faa="{group}/annotation/contigs.faa",
        fna="{group}/annotation/contigs.fna",
        gff="{group}/annotation/contigs.gff",
        cut_faa=expand("{{group}}/annotation/temp_splits/Batch_{nb}.faa",nb=range(100))
    params:
        dir='{group}/annotation'
    priority: 100
    threads:
        1000
    message:"Parallel prodigal run in {input}"
    shell:
        "{SCRIPTS}/Parallel_prodigal.py {threads} {input} -s 100 -o {params.dir} -T {params.dir}/temp_splits"

# ---- get the bed file for the cut up contigs-------------------------------------------
rule cut_contigs:
    input:  fa="{group}/contigs/contigs.fa",
            gff="{group}/annotation/contigs.gff"
    output: contig="{group}/contigs/contigs_C10K.fa",
            Contig_bed="{group}/annotation/contigs_C10K.bed"
    priority: 50
    message:"Use orfs annotation to cut contigs"
    shell:  """{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.contig} {output.Contig_bed}"""


# ------------------------------------------ COG Annotation -------------------------------
# ------- rpsblast on previous batchs 
rule Batch_rpsblast:
    input:   "{group}/{path,a.*/Batch.*}.faa"
    output:  "{group}/{path,a.*/Batch.*}.cogs.tsv"
    log:     "{group}/{path,a.*/Batch.*}_cog.log"
    params:  db=COG_DB
    shell:   """
             rpsblast+ -outfmt '6 qseqid sseqid evalue pident length slen qlen' -evalue 0.00001 -query {input} -db {params.db} -out {output} &>log
             """

#------- select best hit and use criterion : min 5% coverage, min 1e-10 evalue--------------
rule parse_cogs_annotation:
    input:   Batch=expand("{{group}}/annotation/temp_splits/Batch_{nb}.cogs.tsv",nb=range(100))
    output:  cog="{group}/annotation/contigs_best_hits.cogs.tsv",
             cat=temp("{group}/annotation/contigs_Cog.out")
    shell:   """
             cat {input} > {output.cat}   
             {SCRIPTS}/Filter_Cogs.py {output.cat} --cdd_cog_file {SCG_DATA}/cdd_to_cog.tsv  > {output.cog}
             """

# ------- extract scg ------------------------------------------------------------------------
checkpoint extract_SCG_sequences:
    input:  annotation="{filename}_best_hits.cogs.tsv",
            gff="{filename}.gff",
            fna="{filename}.fna"
    output: "{filename}_SCG.fna"
    shell:  "{SCRIPTS}/Extract_SCG.py {input.fna} {input.annotation} {SCG_DATA}/scg_cogs_min0.97_max1.03_unique_genera.txt {input.gff}>{output}"

# --------- Concoct ------------------------------------------------------------------------
def get_initial_number_of_bins(wildcards):
    SCF_faa_handle=open(checkpoints.extract_SCG_sequences.get(filename=wildcards.group+"/annotation/contigs").output[0])
    nb_bin=int(5*np.median(list(Counter([header.split(" ")[1] for header,seq in SimpleFastaParser(SCF_faa_handle)]).values())))
    return min(nb_bin,MAX_BIN_NB)

rule concoct:
    input:   cov="{group}/profile/coverage_contigs_C10K.tsv",
             fasta="{group}/contigs/contigs_C10K.fa",
             SCG="{group}/annotation/contigs_SCG.fna"
    output:  "{group}/binning/concoct/clustering_gt"+str(MIN_CONTIG_SIZE)+".csv",
             Data="{group}/binning/concoct/original_data_gt%d.csv"%MIN_CONTIG_SIZE
    params:  min_contig_size=MIN_CONTIG_SIZE,
             nb_bin=get_initial_number_of_bins
    threads: 20
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b {wildcards.group}/binning/concoct -c {params.nb_bin} -l {params.min_contig_size} -t {threads}
             """

# --------- Get SCG table out of clustering --------------------------------------------------
rule SCG_table:
    input  : bins="{group}/binning/{binner}/clustering_{name}.csv",
             SCG="{group}/annotation/contigs_SCG.fna"
    output : "{group}/binning/{binner}/{name}_SCG_table.csv"
    shell  : "{SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} -t {output}"


# --------- Concoct refine -------------------------------------------------------------------

rule refine:
    input:  bins="{group}/{path}clustering_gt%d.csv"%MIN_CONTIG_SIZE,
            table="{group}/{path}gt%d_SCG_table.csv"%MIN_CONTIG_SIZE,
            SCG="{group}/annotation/contigs_SCG.fna",
            Data="{group}/binning/concoct/original_data_gt%d.csv"%MIN_CONTIG_SIZE
    output: bins="{group}/{path,.*/.*/}clustering_refine.csv",
    params: temp="{group}/{path}refine.temp" 
    log:    temp("{group}/{path,.*/.*/}clustering.log")
    threads: 20
    shell:  """
            ROOTDIR=$(pwd)
            sed '1d' {input.bins}  > {params}
            cd {wildcards.group}/binning/concoct/
            concoct_refine $ROOTDIR/{params} $ROOTDIR/{input.Data} $ROOTDIR/{input.table} -t {threads} &>$ROOTDIR/{log}
            rm $ROOTDIR/{params}
            cd $ROOTDIR
            """

# --------- merge back contigs -------------------------------------------------------------------
rule merge_contigs:
    input:   refine="{path}clustering_refine.csv",
             table ="{path}refine_SCG_table.csv" 
    output:  "{path}clustering_concoct.csv"
    log:     "{path}clustering_consensus.log"
    shell:   "{SCRIPTS}/Consensus.py {input.refine} >{output} 2>{log}"

# --------- estimate the number of mag ------------------------------------------------------------

rule output_number_of_mag:
    input:   table="{path}/{binner}_SCG_table.csv"
    output:  mag_nb="{path}/{binner}_MAG_nb.txt",
             mag_list="{path}/{binner}_MAG_list.txt"
    run:
        with open(output["mag_nb"],"w") as handle_nb:
            with open(output["mag_list"],"w") as handle_list:
                nb=0
                mags=[]
                for index,line in enumerate(open(input["table"])) :
                    if index==0:
                        continue
                    split_line=line.rstrip().split(',')
                    if sum([element=="1" for element in split_line[1:]])>=(0.75*36) :
                        nb+=1
                        mags.append(split_line[0])
                handle_nb.write(str(nb)+"\n")
                handle_list.write("\n".join(["Bin_"+nb for nb in mags]))

# --------- produce a contig file for each bins be they good or not (metabat2 style) ---------------
rule output_bins:
    input:   contigs="{group}/contigs/contigs.fa",
             clustering="{group}/binning/concoct/clustering_concoct.csv"
    output:  touch("{group}/binning/concoct/bins/done")
    params:  "{group}/binning/concoct/bins/"
    shell :"""
    {SCRIPTS}/Split_fasta_by_bin.py {input.contigs} {input.clustering} {params}"""



