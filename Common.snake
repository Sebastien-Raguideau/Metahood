import sys
import yaml
import glob
import os.path
import tempfile
from itertools import chain
from functools import partial
from collections import defaultdict
from psutil import virtual_memory
from subprocess import Popen, PIPE
from snakemake.exceptions import WorkflowError
from os.path import basename,dirname,abspath,realpath
from scripts.common import detect_reads, fill_default_values, extended_glob, replace_extensions, get_extension

#--------------------------- Config parameters ---------------------------
# config=yaml.load(open("./config_AD07.yaml"))
# config["LOCAL_DIR"]="/mnt/gpfs/seb/Project/Metahood/" 
fill_default_values(config)
METAHOOD=dirname(abspath(realpath(workflow.snakefile)))

# --------- Setup ----------
SETUP = 0

# --------- Sample filtering ----------
FILTER = config["filtering"]

# -------- Assembly --------
IN = config["data"]
ASSEMBLER_PARAMS = config["assembly"]["parameters"]

# -------- Ressources --------
MAX_MEM_PERCENT = config["Percent_memory"]
THREADS = config["threads"]
SCRIPTS = config["scripts"]
Mem_tot = virtual_memory().total
MAX_MEMG = int((MAX_MEM_PERCENT*Mem_tot)/10**9)
TASK_MEMORY = config["task_memory"]
CONDA_ENV = config["conda_env"]
CONFIG_PATH = config["CONFIG_PATH"]
LOCAL_DIR = config["LOCAL_DIR"]
EXEC_DIR = config["EXEC_DIR"]

# -------- Binning -----------
# option related to which samples would we want to be mapping to the assembly
# do we want all samples or just the sample used for ssa
# default false, we want all samples
COVERAGE_UNIQUE_SAMPLE = config["binning"]["ssa_unique_sample"]

CONCOCT_EXECUTION = config["binning"]["concoct"]["execution"]
BINNING_TASKS=lambda wildcards: [wildcards.group+""]
MAX_BIN_NB=config["binning"]["concoct"]["max_bin_nb"]
MIN_CONTIG_SIZE = config["binning"]["concoct"]["contig_size"]
METABAT2_EXECUTION = config["binning"]["metabat2"]["execution"]
MIN_CONTIG_SIZE_METABAT2=config["binning"]["metabat2"]["contig_size"]
BINNER=CONCOCT_EXECUTION*["concoct"]+METABAT2_EXECUTION*["metabat2"]
BINNER+=(len(BINNER)-1)*["consensus"]
# -------- Annotation --------
ANNOTATION = config["annotation"]
CHECKM_DB = ANNOTATION["checkm"]

# SCG 
SCG_DATA = config["scg_data"]
SCG_HMM = "%s/hmms/checkm.hmm"%CHECKM_DB
SCG = {line.rstrip().split("\t")[0] for line in open("%s/scg_hmm_selected.txt"%SCG_DATA) if line.rstrip().split("\t")[2]=="fine"}

DIAMOND = ANNOTATION["diamond"]
CAT_DB = ANNOTATION["cat_db"]
KRAKEN_DB = ANNOTATION["kraken_db"] 
KOFAMSCAN = ANNOTATION['kofamscan']
KO_HMM = KOFAMSCAN["profiles"]
KO_HMM_CUTOFFS = KOFAMSCAN["ko_list"]
VIRSORTER_DB = ANNOTATION["virsorter"]
PLASMIDNET = ANNOTATION["plasmidnet_install"]

BEST_HITS = ["KEGG"]*(KO_HMM!="")+[annotation for annotation in DIAMOND]
assert len(BEST_HITS)==len(set(BEST_HITS)), "diamond annotation use KEGG as keyword, this is ambiguous as one of these is already done otherwise."

# -------- MAG analysis ------
MAG_ANALYSIS = config["maganalysis"]
HMM = {"cpr43":"%s/cpr_43_markers.hmm"%SCG_DATA, "ar76":"%s/meren_ar76.hmm"%SCG_DATA, "bac71":"%s/meren_bac71.hmm"%SCG_DATA}
GTDB = config["gtdb"]

# -------- DESMAN ------
DESMAN=config["desman"]["execution"]
DSCRIPTS=config["desman"]["scripts"]
DESMAN_HAPLOTYPE_NB = config["desman"]["nb_haplotypes"]
DESMAN_REPEAT = config["desman"]["nb_repeat"]
MIN_COV_DESMAN = config["desman"]["min_cov"]


# # -------- graph --------
if config["graph"] :
    GRAPH_TASKS=["contigs."+".".join(List_annotation)+"_contiguous_ORF_annotation.csv" for dict_annotation in config["graph"].values() for List_annotation in dict_annotation.values()]
    DB_COLORS={DB:Dict_things['color'] for DB,Dict_things in DIAMOND.items() if "color" in Dict_things}
else :
    GRAPH_TASKS=[]

# ---------- other flag ------
IS_FASTA = 0 # in case metahood is run on fasta files, change a few things

#--------------------------- Read group and sample composition ---------------------------

#Â -------- persample : create groups of one sample --------
if "per_sample" in config["assembly"] :
    SAMPLES=config["assembly"]["per_sample"]
    for sample_regex in SAMPLES :
        prefix=""
        if "|" in sample_regex :
            prefix,sample_regex=sample_regex.split("|")
            prefix+="/"
        for sample in extended_glob(IN+"/"+sample_regex) :
            sample_name=sample.split("/")[-1]
            config["assembly"]["groups"][prefix+sample_name]=[sample_name]

# -------- Groups --------
# define the groups as a dictionary mapping to the path of samples folder
GROUPS_DEF=config["assembly"]["groups"]
GROUPS=defaultdict(list)
for group,list_sample in GROUPS_DEF.items() :
    # get the samples paths corresponding to the group
    GROUPS[group]=[path for sample in list_sample for path in extended_glob(IN+"/"+sample)]

# check that group definition allow us to find samples
Empty_group=[group for group,value in GROUPS.items() if len(value)==0]
if Empty_group :
    raise WorkflowError("Samples specified in groups "+"/".join(Empty_group)+" haven't been found at "+IN+ ", check your config file for potential errors")

# -------- Files associated with each samples --------
# Each sample should have its own folder with fasta/fastq files in it
SAMPLES= {basename(sample) for list_sample in GROUPS.values() for sample in list_sample}

SAMPLE_READS = dict(map(lambda sample: (sample, detect_reads(os.path.join(IN, sample))), SAMPLES))

Files_nb = [sample for sample,list_file in SAMPLE_READS.items() if len(list_file)!=2]
if Files_nb :
    raise WorkflowError("Samples folder : "+" - ".join(Files_nb)+"  does not have exactly 2 reads files, you may have more or less than 2, files recognised as reads files are the following : .fastq, .fastq.gz, .fq, .fq.gz, .fa, .fa.gz, .fasta, .fasta.gz. You may also want to check the regular expression you used to select samples")

# replace_extension, change file name so that trimmed version is taken for assembly/mapping..etc instead of initials samples
R1={sample:replace_extensions(list_reads[0],FILTER) for sample,list_reads in SAMPLE_READS.items()}
R2={sample:replace_extensions(list_reads[1],FILTER) for sample,list_reads in SAMPLE_READS.items()}

# in case all reads are fasta files, skip trimming/fastqc
if len([read for read in [value for value in R1.values()] if get_extension(read) in {".fastq.gz",".fastq",".fq.gz",".fq"}])==0 :
    IS_FASTA=1
    if not os.path.exists("%s/quality_done"%IN) :
        os.system("touch %s/quality_done"%IN)

# Define reads by groups
GROUP_R1={group:[R1[basename(sample)] for sample in list_sample] for group,list_sample in GROUPS.items()}
GROUP_R2={group:[R2[basename(sample)] for sample in list_sample] for group,list_sample in GROUPS.items()}

# Add definition to global dict
R1.update(GROUP_R1)
R2.update(GROUP_R2)




