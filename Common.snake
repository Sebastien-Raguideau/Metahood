import sys
from psutil import virtual_memory
from itertools import chain
from functools import partial
from collections import defaultdict
import glob
import os.path
from os.path import basename,dirname
import tempfile
import yaml
from subprocess import Popen, PIPE
from snakemake.exceptions import WorkflowError
from scripts.common import detect_reads, fill_default_values, extended_glob

#--------------------------- Config parameters ---------------------------

fill_default_values(config)

# --------- Setup ----------
SETUP=config["samples"]["setup"]

# -------- Assembly --------
IN = config["data"]
ASSEMBLER = config["assembly"]["assembler"]
ASSEMBLER_PARAMS = config["assembly"]["parameters"]

# -------- Ressources --------
MAX_MEM_PERCENT = config["Percent_memory"]
THREADS = config["threads"]
SCRIPTS = config["scripts"]
Mem_tot=virtual_memory().total
MAX_MEMG=int((MAX_MEM_PERCENT*Mem_tot)/10**9)

# -------- Binning -----------
CONCOCT_EXECUTION = config["binning"]["concoct"]["execution"]
BINNING_TASKS=lambda wildcards: [wildcards.group+""]
MAX_BIN_NB=config["binning"]["concoct"]["max_bin_nb"]
MIN_CONTIG_SIZE = config["binning"]["concoct"]["contig_size"]
METABAT2_EXECUTION = config["binning"]["metabat2"]["execution"]
MIN_CONTIG_SIZE_METABAT2=config["binning"]["metabat2"]["contig_size"]
BINNER=CONCOCT_EXECUTION*["concoct"]+METABAT2_EXECUTION*["metabat2"]

# -------- Annotation --------
COG_DB= config["cog_database"]
SCG_DATA = config["scg_data"]
ANNOTATION = config["annotation"]

# -------- MAG analysis ------
MAG_ANALYSIS=config["maganalysis"]
LIST_COGS=[line.rstrip() for line in open(SCG_DATA+"/scg_cogs_min0.97_max1.03_unique_genera.txt")]

# -------- DESMAN ------
DESMAN=config["desman"]["execution"]
DSCRIPTS=config["desman"]["scripts"]
DESMAN_HAPLOTYPE_NB = config["desman"]["nb_haplotypes"]
DESMAN_REPEAT = config["desman"]["nb_repeat"]
MIN_COV_DESMAN = config["desman"]["min_cov"]


# -------- graph --------
if config["graph"]["List_graph"] :
    GRAPH_TASKS=["contigs."+".".join(List_annotation)+"_contiguous_ORF_annotation.csv" for List_annotation in config["graph"]["List_graph"].values()]
    CAT_DB=config["graph"]["CAT_database"]
    DB_COLORS={DB:Dict_things['color'] for DB,Dict_things in ANNOTATION.items()}
else :
    GRAPH_TASKS=[]




#--------------------------- Read group and sample composition ---------------------------

#Â -------- persample : create groups of one sample --------
if "per_sample" in config["assembly"] :
    SAMPLES=config["assembly"]["per_sample"]
    for sample_regex in SAMPLES :
        prefix=""
        if "|" in sample_regex :
            prefix,sample_regex=sample_regex.split("|")
            prefix+="/"
        for sample in extended_glob(IN+"/"+sample_regex) :
            sample_name=sample.split("/")[-1]
            config["assembly"]["groups"][prefix+sample_name]=[sample_name]

# -------- Groups --------
# define the groups as a dictionary mapping to the path of samples folder
GROUPS_DEF=config["assembly"]["groups"]
GROUPS=defaultdict(list)
for group,list_sample in GROUPS_DEF.items() :
    # get the samples paths corresponding to the group
    GROUPS[group]=[path for sample in list_sample for path in extended_glob(IN+"/"+sample)]
Empty_group=[group for group,value in GROUPS.items() if len(value)==0]
if Empty_group :
    raise WorkflowError("Samples specified in groups "+"/".join(Empty_group)+" haven't been found at "+IN+ ", check your config file for potential errors")

# -------- Files associated with each samples --------
# Each sample should have its own folder with fasta/fastq files in it
SAMPLES= {sample.split("/")[-1] for list_sample in GROUPS.values() for sample in list_sample}
SAMPLE_READS = dict(map(lambda sample: (sample, detect_reads(os.path.join(IN, sample))), SAMPLES))

Files_nb = [sample for sample,list_file in SAMPLE_READS.items() if len(list_file)!=2]
if Files_nb :
    raise WorkflowError("Samples folder : "+" - ".join(Files_nb)+"  does not have 2 fastq files, check the regular expression you used to select samples")

# Define reads by samples
qual_rep=lambda fastq:fastq.replace("_R1.fastq.gz","_R1_val_1.fq.gz").replace("_R2.fastq.gz","_R2_val_2.fq.gz")
LEFT_READS={sample:qual_rep(list_reads[0]) for sample,list_reads in SAMPLE_READS.items()}
RIGHT_READS={sample:qual_rep(list_reads[1]) for sample,list_reads in SAMPLE_READS.items()}
# Define reads by groups
GROUP_LEFT_READS={group:[LEFT_READS[basename(sample)] for sample in list_sample] for group,list_sample in GROUPS.items()}
GROUP_RIGHT_READS={group:[RIGHT_READS[basename(sample)] for sample in list_sample] for group,list_sample in GROUPS.items()}
# Add definition to global dict
LEFT_READS.update(GROUP_LEFT_READS)
RIGHT_READS.update(GROUP_RIGHT_READS)


def samples_yaml():
    libs = []
    for s in SAMPLES:
        info = {}
        info["left reads"] = [SAMPLE_READS[s][0]]
        info["right reads"] = [SAMPLE_READS[s][1]]
        info["type"] = "paired-end"
        info["orientation"] = "fr"
        libs.append(info)
    return yaml.dump(libs, default_style='"', default_flow_style=False)

def is_fastq(wildcards):
    name = getattr(wildcards, "sample", None)
    if not name:
        try:
            name = GROUPS[wildcards.group][0]
        except KeyError:
            name = wildcards.group

    for ext in {".fasta", ".fasta.gz", ".fa", ".fa.gz", ".fna", ".fna.gz", ".fsa",".fsa.gz", ".fastq", ".fastq.gz", ".fq", ".fq.gz"}:
        if SAMPLE_READS[name][0].endswith(ext):
            return True
    return False

def check_scratch_avail(root):
    if os.path.isdir(root):
        try:
            with tempfile.TemporaryDirectory(dir=root) as temp_dir:
                return True
        except:
            print("Launch with tmpdir failed")
    return False

