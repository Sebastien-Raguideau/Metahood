from Bio.SeqIO.FastaIO import SimpleFastaParser as SFP

rule generate_depth :
    input:  lambda w : [w.group+"/map/"+sample.split('/')[-1]+"_mapped_sorted.bam" for sample in  GROUPS[w.group]]
    output: "{group}/map/depth.txt"
    log : "{group}/map/depth.log"
    shell: "jgi_summarize_bam_contig_depths --outputDepth {output} {input} &>{log}" 

rule metabat2 :
    input:  contig="{group}/contigs/contigs.fa",
            depth="{group}/map/depth.txt"
    output: "{group}/binning/metabat2/bins/done"
    params: out="{group}/binning/metabat2/bins/bin",
            min_contig_size=max(1500,MIN_CONTIG_SIZE_METABAT2) # metabat2 does not bin anything smaller than 1500
    threads : 20
    shell: """metabat2 -i {input.contig} -a {input.depth} -t {threads} -o {params.out} -m {params.min_contig_size}
            touch {output}
            """

rule post_processing :
    input: "{group}/binning/metabat2/bins/name_done"
    output:"{group}/binning/metabat2/clustering_metabat2.csv"
    run:
        List_bins=glob.glob(wildcards.group+"/binning/metabat2/bins/Bin*.fa")
        Handle=open(output[0],"w")
        Handle.write("contig_id,0\n")
        for file in List_bins :
            bin_name=file.split("Bin_")[-1].split('.fa')[0]
            for name,seq in SFP(open(file)) :
                Handle.write(",".join([name,bin_name])+"\n")
        Handle.close()

rule renames_metabat2_bins :
    input: "{group}/binning/metabat2/bins/done"
    output:"{group}/binning/metabat2/bins/name_done"
    run:
        List_bins=glob.glob(wildcards.group+"/binning/metabat2/bins/bin*.fa")
        for bin_ in List_bins:
            new_name=bin_.replace("bin.","Bin_")
            os.system("mv %s %s"%(bin_,new_name))
        os.system("touch %s"%output)
