from Bio.SeqIO.FastaIO import SimpleFastaParser as SFP
from os.path import dirname,basename,join
import gzip

# ---- generate contig bed   ----------------------------------------------------
rule bogus_bed:
    input:   contig="{group}/contigs/contigs.fa"
    output:  bed="{group}/annotation/contigs.bed"
    run :
        handle=open(output['bed'],"w")
        for header,seq in SFP(open(input["contig"])) :
            name=header.split(" ")[0]
            handle.write("\t".join([name,"0",str(len(seq)),name+"\n"]))
        handle.close()

# ---- generate orfs bed   ----------------------------------------------------
rule bed_orfs:
    input:   gff="{path}/contigs.gff"
    output:  bed="{path}/orf.bed"
    shell : "{SCRIPTS}/Gff_to_bed.py {input.gff} {output.bed}"


# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools:
    input:   bam="{group}/map/{sample}_mapped_sorted.bam",
             bed="{group}/annotation/{type}.bed"
    output:  "{group}/map/{sample}.{type}.cov"
    log:      "{group}/map/{sample}_{type}.log"
    resources:
        memG=400
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -mean > {output} 2>{log} "

# ---- collate all files  -----------------------
rule coverage:
    input:   lambda w : [w.group+"/map/"+sample.split('/')[-1]+".{type}.cov" for sample in  GROUPS[w.group]]
    output:  "{group}/profile/coverage_{type}.tsv"
    shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "

# ---- generate a mapping of annotation to orfs  -----------------------
rule map_annotation_to_orfs:
    input:   "{path}/contigs_{annotation}_best_hits.tsv"
    output:  "{path}/map_{annotation}_to_orfs.tsv"
    shell:  """{SCRIPTS}/Annotation_listcontig.py {input} {output} """

# ---- generate annotation profile  -----------------------
rule generate_profile:
    input:  cov="{group}/profile/coverage_orf.tsv",
            map="{group}/annotation/map_{annotation}_to_orfs.tsv"
    output: "{group}/profile/cov_{annotation}.tsv"
    shell:  "{SCRIPTS}/Extract_gene_profile.py {input.map} {input.cov} {output}"

# ----------------------------------------------------------
# ---- generate Normalisation file  -----------------------
# ----------------------------------------------------------

# ---- generate nb nucleotides by sample  -----------------------
if IS_FASTA : 
    rule nb_bases_fasta:
        input: lambda w : [fastq_file for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        run: 
            dict_sample_nb=defaultdict(list)
            for file in input:
                sample=basename(dirname(file))
                if file.endswith(".gz") :
                    handle = gzip.open(file,"rt")
                else :
                    handle = open(file)
                nb = sum([len(seq) for header,seq in sfp(handle)])
                dict_sample_nb[sample].append(nb)
            dict_sample_nb={sample:str(sum(list_nb)) for sample,list_nb in dict_sample_nb.items()}
            with open(output[0],"w") as handle:
                handle.write("Normalisation\t"+"\t".join(dict_sample_nb.keys())+"\n")
                handle.write("Nb_nucleotides\t"+"\t".join(dict_sample_nb.values())+"\n")
            handle.close()
else :
    rule nb_bases:
        input: lambda w : [fastq_file+"_trimming_report.txt" for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        run: 
            dict_sample_nb=defaultdict(list)
            for file in input:
                sample=basename(dirname(file))
                with open(file) as handle:
                    line=next(handle)
                    while "Total written (filtered):" not in line:
                        line=next(handle)
                    nb=line.split(":")[1].split("bp")[0].replace(",","").replace(" ","")
                    dict_sample_nb[sample].append(float(nb))
            dict_sample_nb={sample:str(sum(list_nb)) for sample,list_nb in dict_sample_nb.items()}
            print(output[0])
            with open(output[0],"w") as handle:
                handle.write("Normalisation\t"+"\t".join(dict_sample_nb.keys())+"\n")
                handle.write("Nb_nucleotides\t"+"\t".join(dict_sample_nb.values())+"\n")
            handle.close()


# ---- generate cog profile  -----------------------

rule median_SCG_cov:
    input:  cov="{group}/profile/cov_cogs.tsv",
            nuc="{group}/profile/nucleotides.tsv"
    output: "{group}/profile/Normalisation.tsv"
    run: 
        set_SCG = {cog.rstrip() for cog in open(SCG_DATA+"/scg_cogs_min0.97_max1.03_unique_genera.txt")}
        List_profile = []
        # compute median of SCG       
        with open(input.cov) as handle:
            samples = next(handle).rstrip().split("\t")[1:]
            for index,line in enumerate(handle):
                split_line = line.rstrip().split("\t")
                cog = split_line[0]
                if cog in set_SCG:
                    List_profile.append([float(element) for element in split_line[1:]])
        scg_norm=np.median(List_profile, axis=0)
        # get previous normalisation
        with open(input.nuc) as handle:
            samples_local=next(handle).rstrip().split("\t")[1:]
            nuc_norm=next(handle).rstrip().split("\t")[1:]
        sample_to_norm={sample:nuc_norm[index] for index,sample in enumerate(samples_local)}
        with open(output[0],"w") as handle:
            handle.write("Normalisation\t"+"\t".join(samples)+"\n")
            handle.write("Nucleotides\t"+"\t".join([sample_to_norm[sample] for sample in samples])+"\n")
            handle.write("median_scg\t"+"\t".join(map(str,scg_norm))+"\n")


# ---- percent maped info  -----------------------

rule mapped_reads:
    input:  bam="{group}/map/{sample}_mapped_sorted.bam"
    output: readnb="{group}/map/{sample}_mapped_read.txt"
    threads:THREADS
    shell: """samtools flagstat {input.bam} -@ {threads} | grep 'paired in sequencing' | cut -f1 -d " " > {output.readnb}""" 

if IS_FASTA :
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(( $(zgrep -c '>' {input.R1})*2 )) > {output}"
else : 
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(($(zless {input.R1} | wc -l)/2)) > {output}"

rule get_percent:
    input: mapped=lambda w:[w.group+"/map/"+sample.split('/')[-1]+"_mapped_read.txt" for sample in  GROUPS[w.group]],
           total=lambda w:[IN+"/"+sample.split('/')[-1]+"/"+sample.split('/')[-1]+"_readsnb.txt" for sample in  GROUPS[w.group]]
    output:  "{group}/profile/mapping_percent.tsv"
    shell: """
            for path in {input.mapped}
            do
                file=$(basename $path)
                sample=${{file%_mapped_read.txt}}
                tot_reads=$(cat {IN}/$sample/$sample"_readsnb.txt")
                mapped_reads=$(cat $path)
                echo $tot_reads $mapped_reads
                echo -e $sample\t$(bc -l <<< "$mapped_reads / $tot_reads")>>{output}
            done
           """


# ---- mags infos  ----------------------
def get_cluster_def(file,mags):
    cluster_def=defaultdict(list)
    contig_to_cluster={}
    for line in open(file):
        contig,cluster=line.rstrip().split(",")
        if cluster in mags:
            cluster_def[cluster].append(contig)
            contig_to_cluster[contig]=cluster
    return cluster_def,contig_to_cluster

def get_contig_cov(file,contigs):
    contig_cov={}
    with open(file) as handle :
        header=next(handle)
        for line in handle:
            splitline=line.rstrip().split("\t")
            contig=splitline[0]
            if contig in contigs:
                contig_cov[contig]=np.array(list(map(float,splitline[1:])))
    return header,contig_cov

rule mag_coverage:
    input: mags="{group}/binning/{binner}/{binner}_MAG_list.txt",
           cluster="{group}/binning/{binner}/clustering_{binner}.csv",
           cov="{group}/profile/coverage_contigs.tsv",
           len="{group}/annotation/contigs.bed",
           nb_nuc="{group}/profile/Normalisation.tsv"
    output: mag_cov="{group}/profile/mag_{binner}_coverage.tsv",
            mag_map="{group}/profile/mag_{binner}_percent_mapped.tsv"
    run:
        # get the sets of bins, considered mags
        mags={line.rstrip() for line in open(input["mags"])}
        # get cluster definition
        cluster_def,contig_to_cluster=get_cluster_def(input["cluster"],mags)
        # get contigs coverage
        header,contig_cov=get_contig_cov(input["cov"],contig_to_cluster)
        # get contigs lengths
        contig_len={line.rstrip().split('\t')[0]:float(line.rstrip().split('\t')[2]) for line in open(input["len"]) if line.rstrip().split('\t')[0] in contig_to_cluster}
        # get nb reads :
        with open(input["nb_nuc"]) as handle:
            header_norm = next(handle).rstrip().split("\t")[1:]
            samples = header.rstrip().split('\t')[1:]
            nuc_nb=list(map(float,next(handle).rstrip().split("\t")[1:]))
            if samples != header_norm :
                reorder = [header_norm.index(elmt) for elmt in samples]
                header_norm = np.array(header_norm)[reorder]
                nuc_nb = np.array(nuc_nb)[reorder]
            assert samples == header_norm,"error, header are differents : \n%s\n%s"%(samples, header_norm)

        # get mag nucleotides
        handle_cov=open(output["mag_cov"],"w")
        handle_map=open(output["mag_map"],"w")
        handle_map.write("mag\t"+"\t".join(samples)+"\n")
        handle_cov.write("mag\t"+"\t".join(samples)+"\n")
        count = 0
        for cluster,list_contigs in cluster_def.items():
            if cluster =="0":
                print("Bin_%s\t"%cluster,count)
            tot_len=0
            tot_nuc=np.zeros(len(samples))
            for contig in list_contigs:
                if contig=="contig_id":
                    continue
                tot_len+=contig_len[contig]
                tot_nuc+=contig_cov[contig]*contig_len[contig]
            handle_cov.write("Bin_%s\t"%cluster+"\t".join(list(map(str,tot_nuc/tot_len)))+"\n")
            handle_map.write("Bin_%s\t"%cluster+"\t".join(list(map(str,tot_nuc/nuc_nb)))+"\n")
            count+=1
        handle_cov.close()
        handle_map.close()







