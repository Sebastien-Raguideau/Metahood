#from Bio.SeqIO.FastaIO import SimpleFastaParser as SFP
from os.path import dirname,basename,join
#import gzip

# ---- generate contig bed   ----------------------------------------------------
rule bogus_bed:
    input:   contig="{group}/contigs/contigs.fa"
    output:  bed="{group}/annotation/contigs.bed"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell : "{SCRIPTS}/bogus_bed.py -i {input.contig} -o {output.bed}"

# ---- generate orfs bed   ----------------------------------------------------
rule bed_orfs:
    input:   gff="{path}/contigs.gff"
    output:  bed="{path}/orf.bed"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell : "{SCRIPTS}/Gff_to_bed.py {input.gff} {output.bed}"


# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools:
    input:   bam="{group}/map/{sample}_mapped_sorted.bam",
             bed="{group}/annotation/{type}.bed"
    output:  "{group}/map/{sample}.{type}.cov"
    log:      "{group}/map/{sample}_{type}.log"
    resources:
        memG=TASK_MEMORY
    conda : CONDA_ENV + "/bedtools.yaml"
    singularity : "builds/bedtools.sif"
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -mean > {output} 2>{log} "

# ---- collate all files  -----------------------
rule coverage:
    input:   lambda w : [w.group+"/map/"+sample.split('/')[-1]+".{type}.cov" for sample in  GROUPS[w.group]]
    output:  "{group}/profile/coverage_{type}.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "

# ---- generate a mapping of annotation to orfs  -----------------------
rule map_annotation_to_orfs:
    input:   "{path}/contigs_{annotation}_best_hits.tsv"
    output:  "{path}/map_{annotation}_to_orfs.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell:  """{SCRIPTS}/Annotation_listcontig.py {input} {output} """

# ---- generate annotation profile  -----------------------
rule generate_profile:
    input:  cov="{group}/profile/coverage_orf.tsv",
            map="{group}/annotation/map_{annotation}_to_orfs.tsv"
    output: "{group}/profile/cov_{annotation}.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell:  "{SCRIPTS}/Extract_gene_profile.py {input.map} {input.cov} {output}"

# ----------------------------------------------------------
# ---- generate Normalisation file  -----------------------
# ----------------------------------------------------------

# ---- generate nb nucleotides by sample  -----------------------
if IS_FASTA : 
    rule nb_bases_fasta:
        input: lambda w : [fastq_file for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        conda : CONDA_ENV + "/pythonenv.yaml"
        singularity : "builds/pythonenv.sif"
        script: "{SCRIPTS}/nb_bases_fasta.py {input} {output}"
            
else :
    rule nb_bases:
        input: lambda w : [fastq_file+"_trimming_report.txt" for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        run: 
            dict_sample_nb=defaultdict(list)
            for file in input:
                sample=basename(dirname(file))
                with open(file) as handle:
                    line=next(handle)
                    while "Total written (filtered):" not in line:
                        line=next(handle)
                    nb=line.split(":")[1].split("bp")[0].replace(",","").replace(" ","")
                    dict_sample_nb[sample].append(float(nb))
            dict_sample_nb={sample:str(sum(list_nb)) for sample,list_nb in dict_sample_nb.items()}
            print(output[0])
            with open(output[0],"w") as handle:
                handle.write("Normalisation\t"+"\t".join(dict_sample_nb.keys())+"\n")
                handle.write("Nb_nucleotides\t"+"\t".join(dict_sample_nb.values())+"\n")
            handle.close()


# ---- generate cog profile  -----------------------

rule median_SCG_cov:
    input:  cov="{group}/profile/cov_cogs.tsv",
            nuc="{group}/profile/nucleotides.tsv"
    output: "{group}/profile/Normalisation.tsv"
    params: scgdata=SCG_DATA
    conda: CONDA_ENV + "/pythonenv.yaml"
    singularity: "builds/pythonenv.sif"
    shell : """
    {SCRIPTS}/median_scg.py -c {input.cov} -n {input.nuc} -o {output} -s {params.scgdata}
    """

# ---- percent maped info  -----------------------

rule mapped_reads:
    input:  bam="{group}/map/{sample}_mapped_sorted.bam"
    output: readnb="{group}/map/{sample}_mapped_read.txt"
    threads:THREADS
    conda : CONDA_ENV + "/bwasamtools.yaml"
    singularity : "builds/bwasamtools.sif"
    shell: """samtools flagstat {input.bam} -@ {threads} | grep 'paired in sequencing' | cut -f1 -d " " > {output.readnb}""" 

if IS_FASTA :
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(( $(zgrep -c '>' {input.R1})*2 )) > {output}"
else : 
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(($(zless {input.R1} | wc -l)/2)) > {output}"

rule get_percent:
    input: mapped=lambda w:[w.group+"/map/"+sample.split('/')[-1]+"_mapped_read.txt" for sample in  GROUPS[w.group]],
           total=lambda w:[IN+"/"+sample.split('/')[-1]+"/"+sample.split('/')[-1]+"_readsnb.txt" for sample in  GROUPS[w.group]]
    output:  "{group}/profile/mapping_percent.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell: """
            for path in {input.mapped}
            do
                file=$(basename $path)
                sample=${{file%_mapped_read.txt}}
                tot_reads=$(cat {IN}/$sample/$sample"_readsnb.txt")
                mapped_reads=$(cat $path)
                echo $tot_reads $mapped_reads
                echo -e $sample\t$(bc -l <<< "$mapped_reads / $tot_reads")>>{output}
            done
           """


# ---- mags infos  ----------------------

rule mag_coverage:
    input: mags="{group}/binning/{binner}/{binner}_MAG_list.txt",
           cluster="{group}/binning/{binner}/clustering_{binner}.csv",
           cov="{group}/profile/coverage_contigs.tsv",
           len="{group}/annotation/contigs.bed",
           nb_nuc="{group}/profile/Normalisation.tsv"
    output: mag_cov="{group}/profile/mag_{binner}_coverage.tsv",
            mag_map="{group}/profile/mag_{binner}_percent_mapped.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "builds/pythonenv.sif"
    shell:"""
    {SCRIPTS}/mag_coverage.py -m {input.mags} -c {input.cluster} -t {input.cov} -l {input.len} -n {input.nb_nuc} -v {output.mag_cov} -p {output.mag_map}
    """
