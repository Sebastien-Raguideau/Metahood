#from Bio.SeqIO.FastaIO import SimpleFastaParser as SFP
from os.path import dirname,basename,join
from functools import partial
#import gzip

# ---- generate contig bed   ----------------------------------------------------
rule bogus_bed:
    input:   contig="{group}/contigs/contigs.fa"
    output:  bed=temp("{group}/annotation/contigs.bedtemp")
    conda : CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell : "{SCRIPTS}/bogus_bed.py -i {input.contig} -o {output.bed}"

# ---- generate orfs bed   ----------------------------------------------------
rule bed_orfs:
    input:   gff = "{path}/contigs.gff"
    output:  bed = temp("{path}/orf.bedtemp")
    conda : CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell : "{SCRIPTS}/Gff_to_bed.py {input.gff} {output.bed}"

rule sort_bed:
    input: bed = "{path}/annotation/{type}.bedtemp",
           cont = "{path}/contigs/contigs.fa"
    output: bed = "{path}/annotation/{type}.bed",
            gfile = temp("{path}/annotation/{type}_bedtools_target_definition.tsv")
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    conda : "%s/pythonenv.yaml"%CONDA_ENV
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell: "{SCRIPTS}/sort_bed.py {input.bed} {input.cont} {output.bed} -g {output.gfile}"


# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools:
    input:   bam = "{group}/map/{sample}_mapped_sorted.bam",
             bed = "{group}/annotation/{type}.bed",
             genome = "{group}/annotation/{type}_bedtools_target_definition.tsv"
    output:  "{group}/map/{sample}.{type}.cov"
    log:      "{group}/map/{sample}_{type}.log"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    conda : CONDA_ENV + "/bedtools.yaml"
    singularity : "docker://quay.io/annacprice/bedtools:2.29.2"
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -g {input.genome} -mean -sorted> {output} 2>{log} "

# ---- collate all files  -----------------------

if COVERAGE_UNIQUE_SAMPLE:
    rule coverage:
        input:   lambda w : ["%s/map/%s.{type}.cov"%(w.group,sample.split('/')[-1]) for sample in  GROUPS[w.group]]
        output:  "{group}/profile/coverage_{type}.tsv"
        conda : CONDA_ENV + "/pythonenv.yaml"
        resources:
            slurm_partition = get_resource("partition",1),
            mem_mb = get_resource("mem",1)
        singularity : "docker://quay.io/annacprice/pythonenv:3.9"
        shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "
else:
    rule coverage:
        input:   expand("{{group}}/map/{sample}.{{type}}.cov",sample=SAMPLES)
        output:  "{group}/profile/coverage_{type}.tsv"
        conda : CONDA_ENV + "/pythonenv.yaml"
        resources:
            slurm_partition = get_resource("partition",1),
            mem_mb = get_resource("mem",1)
        singularity : "docker://quay.io/annacprice/pythonenv:3.9"
        shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "

# ---- generate a mapping of annotation to orfs  -----------------------
rule map_annotation_to_orfs:
    input:   "{path}/contigs_{annotation}_best_hits.tsv"
    output:  "{path}/map_{annotation}_to_orfs.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell:  """{SCRIPTS}/Annotation_listcontig.py {input} {output} """

# ---- generate annotation profile  -----------------------
rule generate_profile:
    input:  cov="{group}/profile/coverage_orf.tsv",
            map="{group}/annotation/map_{annotation}_to_orfs.tsv"
    output: "{group}/profile/cov_{annotation}.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell:  "{SCRIPTS}/Extract_gene_profile.py {input.map} {input.cov} {output}"

# ----------------------------------------------------------
# ---- generate Normalisation file  -----------------------
# ----------------------------------------------------------
def filtered(file,FILTER):
    if FILTER:
        file = "%s/Filtered_%s"%(dirname(file),basename(file))
    return(file)


# ---- generate nb nucleotides by sample  -----------------------
if IS_FASTA : 
    rule nb_bases_fasta:
        input: lambda w : [fastq_file for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        conda : CONDA_ENV + "/pythonenv.yaml"
        resources:
            slurm_partition = get_resource("partition",1),
            mem_mb = get_resource("mem",1)
        singularity : "docker://quay.io/annacprice/pythonenv:3.9"
        script: "{SCRIPTS}/nb_bases_fasta.py {input} {output}"
            
else :
    rule nb_bases:
        input: lambda w : [filtered(fastq_file,FILTER)+"_trimming_report.txt" for sample in  GROUPS[w.group] for fastq_file in SAMPLE_READS[basename(sample)]]
        output: temp("{group}/profile/nucleotides.tsv")
        resources:
            slurm_partition = get_resource("partition",1),
            mem_mb = get_resource("mem",1)
        run: 
            dict_sample_nb=defaultdict(list)
            for file in input:
                sample=basename(dirname(file))
                with open(file) as handle:
                    line=next(handle)
                    while "Total written (filtered):" not in line:
                        line=next(handle)
                    nb=line.split(":")[1].split("bp")[0].replace(",","").replace(" ","")
                    dict_sample_nb[sample].append(float(nb))
            dict_sample_nb={sample:str(sum(list_nb)) for sample,list_nb in dict_sample_nb.items()}
            print(output[0])
            with open(output[0],"w") as handle:
                handle.write("Normalisation\t"+"\t".join(dict_sample_nb.keys())+"\n")
                handle.write("Nb_nucleotides\t"+"\t".join(dict_sample_nb.values())+"\n")
            handle.close()


# ---- generate cog profile  -----------------------

rule median_SCG_cov:
    input:  cov="{group}/profile/cov_cogs.tsv",
            nuc="{group}/profile/nucleotides.tsv"
    output: "{group}/profile/Normalisation.tsv"
    params: scgdata=SCG_DATA
    conda: CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity: "docker://quay.io/annacprice/pythonenv:3.9"
    shell : """
    {SCRIPTS}/median_scg.py -c {input.cov} -n {input.nuc} -o {output} -s {params.scgdata}
    """

# ---- percent maped info  -----------------------

rule mapped_reads:
    input:  bam="{group}/map/{sample}_mapped_sorted.bam"
    output: readnb="{group}/map/{sample}_mapped_read.txt"
    threads: 8
    conda : CONDA_ENV + "/bwasamtools.yaml"
    resources:
        slurm_partition = get_resource("partition",8),
        mem_mb = get_resource("mem",8)
    singularity : "docker://quay.io/annacprice/bwasamtools:1.10"
    shell: """samtools flagstat {input.bam} -@ {threads} | grep 'paired in sequencing' | cut -f1 -d " " > {output.readnb}""" 

if IS_FASTA :
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(( $(zgrep -c '>' {input.R1})*2 )) > {output}"
else : 
    rule reads_qty:
        input:  R1=lambda w:R1[w["sample"]]
        output: IN+"/{sample}/{sample}_readsnb.txt"
        shell: "echo $(($(zcat {input.R1} | wc -l)/2)) > {output}"

rule get_percent:
    input: mapped=lambda w:[w.group+"/map/"+sample.split('/')[-1]+"_mapped_read.txt" for sample in  GROUPS[w.group]],
           total=lambda w:[IN+"/"+sample.split('/')[-1]+"/"+sample.split('/')[-1]+"_readsnb.txt" for sample in  GROUPS[w.group]]
    output:  "{group}/profile/mapping_percent.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell: """
            for path in {input.mapped}
            do
                file=$(basename $path)
                sample=${{file%_mapped_read.txt}}
                tot_reads=$(cat {IN}/$sample/$sample"_readsnb.txt")
                mapped_reads=$(cat $path)
                echo $tot_reads $mapped_reads
                echo -e $sample\t$(bc -l <<< "$mapped_reads / $tot_reads")>>{output}
            done
           """


# ---- mags infos  ----------------------

rule mag_coverage:
    input: mags="{group}/binning/{binner}/{binner}_MAG_list.txt",
           cluster="{group}/binning/{binner}/clustering_{binner}.csv",
           cov="{group}/profile/coverage_contigs.tsv",
           len="{group}/annotation/contigs.bed",
           nb_nuc="{group}/profile/Normalisation.tsv"
    output: mag_cov="{group}/profile/mag_{binner}_coverage.tsv",
            mag_map="{group}/profile/mag_{binner}_percent_mapped.tsv"
    conda : CONDA_ENV + "/pythonenv.yaml"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/pythonenv:3.9"
    shell:"""
    {SCRIPTS}/mag_coverage.py -m {input.mags} -c {input.cluster} -t {input.cov} -l {input.len} -n {input.nb_nuc} -v {output.mag_cov} -p {output.mag_map}
    """

# ---------------------- look at ducplicates ----------------------
# (PICARD)

rule picard_duplicate:
    input: bam = "{group}/map/{sample}_mapped_sorted.bam",
           # index = "{group}/map/{sample}_mapped_sorted.bam.bai",
    output: bam = "{group}/map/{sample}_mapped_sorted_dup.bam",
            stat = "{group}/map/{sample}_marked_dup_metrics.txt"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1,mult=10)
    singularity : "docker://biocontainers/picard-tools:v2.18.25dfsg-2-deb_cv1"
    shell: "PicardCommandLine MarkDuplicates I={input.bam} O={output.bam} M={output.stat}"

# samtools
rule samtools_check_duplicate:
    input: bam = "{group}/map/{sample}_mapped_sorted.bam",
           # index = "{group}/map/{sample}_mapped_sorted.bam.bai",
    output: bam = "{group}/map/{sample}_mapped_sorted_dup_samtools.bam",
            stat = "{group}/map/{sample}_marked_dup_metrics_samtools.txt"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1,mult=10)
    singularity : "docker://quay.io/annacprice/bwasamtools:1.10"
    shell: """
    samtools sort -n {input.bam} | samtools fixmate -m - - | samtools sort - | samtools markdup -r -f {output.stat} - {output.bam}
    """

# aggregate info on duplicates
rule check_duplicates:
    input: lambda w : ["%s/map/%s_marked_dup_metrics.txt"%(w.group,sample.split('/')[-1]) for sample in  GROUPS[w.group]]
    output: "{group}/profile/percent_duplicate.tsv"
    shell: "touch {output}"

# ---------------------- get cov hist and cov variation ----------------------
rule index:
    input: "{path}.bam"
    output: "{path}.bam.bai"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "docker://quay.io/annacprice/bwasamtools:1.10"
    shell: "samtools index {input}"

rule get_cov_stats:
    input: bam = "{group}/map/{sample}_mapped_sorted.bam",
           bai = "{group}/map/{sample}_mapped_sorted.bam.bai"
    output: cov_stat = "{group}/map/{sample}_cov.stats",
            cov_hists = "{group}/map/{sample}_cov.hists"
    resources:
        slurm_partition = get_resource("partition",1),
        mem_mb = get_resource("mem",1)
    singularity : "%s/../singularity_container/pythonenv_pysam.sif"%SCRIPTS
    shell: "{SCRIPTS}/CovEvaluate.py {input.bam} {output.cov_stat} {output.cov_hists} -l 10000"


